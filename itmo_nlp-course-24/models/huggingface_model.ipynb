{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67990dc-2ebe-4a6d-a104-cdf52cc8d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260fa6ca-4e7f-41d8-80a2-28c34c5c629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef806bb-2c4e-460e-985b-69e25911d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.exceptions import UndefinedMetricWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efd9f077-466e-475c-926f-c6c03679ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from Levenshtein import distance as levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5109b6e0-b9ce-4f93-8d1e-5a4d8fd048fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a63a5a3-24b5-4a5b-bd70-a25fa8c13f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3723cdd4-3479-47fa-93ad-1254f2d42732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59f949-d38c-43ba-808a-b1c2e868a821",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea33e80d-d55f-48c2-a283-f1590f451eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45f5bf1-2006-4efe-8e46-64781b31bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "lables_fs = 16\n",
    "ticks_fs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5eed0-70df-4ebe-93b7-043416ea0dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07735edc-f07c-46f2-831b-5cdefc5d92c8",
   "metadata": {},
   "source": [
    "## Prepare `train` and `test` datasets from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a5a7bce-8d67-4c98-8a84-8c1e91dc0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dir = '../data/prepared'\n",
    "filename_csv = '01_punct_pushkin.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "24d63ad3-3b17-4217-aefb-c25a5737672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4456, 4)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved dataset\n",
    "dataset_df = pd.read_csv(os.path.join(prepared_dir, filename_csv), index_col=0)\n",
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36f43c88-e94e-4f61-8b3a-603c032b9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.drop('input_lemma', axis=1)\n",
    "dataset_df = dataset_df.drop('input_pos', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "817c92f2-5510-429d-86fc-e62b722298f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNC_2_TOKEN = {' ': 'S', ',': 'C', '.': 'P', '!': 'EX', '?': 'Q'}\n",
    "TOKEN_2_PUNC = {v: k for k, v in PUNC_2_TOKEN.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0daaf47c-f623-4986-a057-e0ebedfe2a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4456/4456 [00:00<00:00, 27831.07it/s]\n"
     ]
    }
   ],
   "source": [
    "output_targets = []\n",
    "\n",
    "for index, row in tqdm(dataset_df.iterrows(), total=dataset_df.shape[0]):\n",
    "    sent_with_punc = ''\n",
    "    for word, token in zip(row['input'].split(), row['target'].split()):\n",
    "        sent_with_punc = sent_with_punc + word + TOKEN_2_PUNC[token]\n",
    "\n",
    "    output_targets.append(sent_with_punc)\n",
    "\n",
    "dataset_df['output_target'] = output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fb26d3f5-150a-4193-88f4-4609773ed8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>output_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>церковь полна была кистеневскими крестьянами пришедшими отдать последнее поклонение господину своему</td>\n",
       "      <td>S S S S C S S S S S P</td>\n",
       "      <td>церковь полна была кистеневскими крестьянами,пришедшими отдать последнее поклонение господину своему.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>муромский провозгласивший себя отличным наездником дал ей волю и внутренне доволен был случаем избавлявшим его от неприятного собеседника</td>\n",
       "      <td>C S S S C S S S S S S S C S S S S P</td>\n",
       "      <td>муромский,провозгласивший себя отличным наездником,дал ей волю и внутренне доволен был случаем,избавлявшим его от неприятного собеседника.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>доложили что мусье давал мне свой урок</td>\n",
       "      <td>C S S S S S P</td>\n",
       "      <td>доложили,что мусье давал мне свой урок.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>мы простились еще раз и лошади поскакали</td>\n",
       "      <td>S S S C S S P</td>\n",
       "      <td>мы простились еще раз,и лошади поскакали.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>начали очищать ров окружающий город а вал обносить рогатками</td>\n",
       "      <td>S S C S C S S S P</td>\n",
       "      <td>начали очищать ров,окружающий город,а вал обносить рогатками.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          input  \\\n",
       "2960                                       церковь полна была кистеневскими крестьянами пришедшими отдать последнее поклонение господину своему   \n",
       "4364  муромский провозгласивший себя отличным наездником дал ей волю и внутренне доволен был случаем избавлявшим его от неприятного собеседника   \n",
       "10                                                                                                       доложили что мусье давал мне свой урок   \n",
       "3752                                                                                                   мы простились еще раз и лошади поскакали   \n",
       "1996                                                                               начали очищать ров окружающий город а вал обносить рогатками   \n",
       "\n",
       "                                   target  \\\n",
       "2960                S S S S C S S S S S P   \n",
       "4364  C S S S C S S S S S S S C S S S S P   \n",
       "10                          C S S S S S P   \n",
       "3752                        S S S C S S P   \n",
       "1996                    S S C S C S S S P   \n",
       "\n",
       "                                                                                                                                   output_target  \n",
       "2960                                       церковь полна была кистеневскими крестьянами,пришедшими отдать последнее поклонение господину своему.  \n",
       "4364  муромский,провозгласивший себя отличным наездником,дал ей волю и внутренне доволен был случаем,избавлявшим его от неприятного собеседника.  \n",
       "10                                                                                                       доложили,что мусье давал мне свой урок.  \n",
       "3752                                                                                                   мы простились еще раз,и лошади поскакали.  \n",
       "1996                                                                               начали очищать ров,окружающий город,а вал обносить рогатками.  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "dataset_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c933ef-0a27-4130-b6c0-364752616125",
   "metadata": {},
   "source": [
    "## Model from Hugging Face\n",
    "\n",
    "We will use [the token classification model](https://huggingface.co/markusiko/rubert-base-punctuation) from Hugging Face for russian punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "65ceb0e9-3220-499a-af62-f6938732370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ec8e0d8-7d11-40ab-b3ae-0d6b167a68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'markusiko/rubert-base-punctuation'\n",
    "\n",
    "model_pretrained = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "352903d5-616e-4351-b72e-16dfe79c91c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba658d6d47a480b9806713dfc6e1748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c6255fde194a959b4450adc2feab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "punct_pipeline = pipeline('token-classification', model=model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfdea3-3d62-43b4-a0d9-0aefcdb77ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8beb6c44-545a-492a-ac83-08837653b683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-,',\n",
       "  'score': 0.94763273,\n",
       "  'index': 9,\n",
       "  'word': 'дам',\n",
       "  'start': 36,\n",
       "  'end': 39},\n",
       " {'entity': 'B-,',\n",
       "  'score': 0.98577046,\n",
       "  'index': 10,\n",
       "  'word': 'разумеется',\n",
       "  'start': 40,\n",
       "  'end': 50},\n",
       " {'entity': 'B-.',\n",
       "  'score': 0.9234911,\n",
       "  'index': 13,\n",
       "  'word': 'пистолетов',\n",
       "  'start': 63,\n",
       "  'end': 73}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'в тридцати шагах промаху в карту не дам разумеется из знакомых пистолетов'\n",
    "\n",
    "sent_punc_preds = punct_pipeline(sent)  # prediction\n",
    "sent_punc_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6ee46842-fab4-4909-9370-07900caebb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the output readable + in target form\n",
    "ALL_PUNC = '.,:;!?'\n",
    "UNK_TOKEN = 'U'\n",
    "\n",
    "\n",
    "def get_sentence_with_punctuation(sent, model_preds, all_entities=[]):\n",
    "\n",
    "    sent_words = sent.split()\n",
    "    sent_new = (sent + ' ')[:-1]\n",
    "    \n",
    "    n_marks = 0\n",
    "    for pred in model_preds:\n",
    "        if pred['entity'] not in all_entities:\n",
    "            all_entities.append(pred['entity'])\n",
    "\n",
    "        punc_mark = pred['entity'][-1]\n",
    "\n",
    "        ind_to_place = pred['end'] + n_marks\n",
    "        if len(sent_new) == ind_to_place:  # last mark\n",
    "            sent_new = sent_new + punc_mark\n",
    "        else:\n",
    "            if (sent_new[ind_to_place] == ' '):\n",
    "                sent_new = sent_new[:ind_to_place] + punc_mark + sent_new[ind_to_place:]\n",
    "                n_marks += 1\n",
    "\n",
    "    target_like = []\n",
    "    for word in sent_new.split():\n",
    "        last_chr = word[-1]\n",
    "        if last_chr in ALL_PUNC:\n",
    "            if last_chr in PUNC_2_TOKEN.keys():\n",
    "                target_like.append(PUNC_2_TOKEN[last_chr])\n",
    "            else:\n",
    "                target_like.append(UNK_TOKEN)\n",
    "        else:\n",
    "            target_like.append(PUNC_2_TOKEN[' '])\n",
    "\n",
    "    return sent_new, ' '.join(target_like), all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "433c7e32-8b64-4061-b3a2-0b6b1e610724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original    : в тридцати шагах промаху в карту не дам разумеется из знакомых пистолетов\n",
      "Processed   : в тридцати шагах промаху в карту не дам, разумеется, из знакомых пистолетов.\n",
      "Target-like : S S S S S S S C C S S P\n"
     ]
    }
   ],
   "source": [
    "print(f'Original    : {sent}')\n",
    "print(f'Processed   : {get_sentence_with_punctuation(sent, sent_punc_preds)[0]}')\n",
    "print(f'Target-like : {get_sentence_with_punctuation(sent, sent_punc_preds)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8e429-7e85-4349-8b85-e009baa54fd1",
   "metadata": {},
   "source": [
    "### Predict all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e17a0297-3e9c-4811-8589-d882b3ab4856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4456/4456 [03:51<00:00, 19.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model_preds = []\n",
    "model_preds_as_target = []\n",
    "\n",
    "all_entities = []\n",
    "\n",
    "for index, row in tqdm(dataset_df.iterrows(), total=dataset_df.shape[0]):\n",
    "    sent_with_punc, target_like, all_entities = get_sentence_with_punctuation(\n",
    "        row['input'], punct_pipeline(row['input']), all_entities\n",
    "    )\n",
    "    model_preds.append(sent_with_punc)\n",
    "    model_preds_as_target.append(target_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b91a64e9-46b8-46e5-a4b2-d285379e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-,', 'B-.', 'B-?', 'B-!', 'B-...']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities  # all entities of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b225e0ba-d498-4ab2-aa9c-f0921dea3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['output'] = model_preds\n",
    "dataset_df['output_punc'] = model_preds_as_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "44feec5f-aac3-4e78-ab49-5c5e8bca37fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>output_target</th>\n",
       "      <th>output</th>\n",
       "      <th>output_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>через полчаса маша должна была навсегда оставить родительский дом свою комнату тихую девическую жизнь</td>\n",
       "      <td>S S S S S S S S C S C S S P</td>\n",
       "      <td>через полчаса маша должна была навсегда оставить родительский дом,свою комнату,тихую девическую жизнь.</td>\n",
       "      <td>через полчаса маша должна была навсегда оставить родительский дом, свою комнату, тихую, девическую жизнь.</td>\n",
       "      <td>S S S S S S S S C S C C S P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>меня привезли в крепость уцелевшую посереди сгоревшего города</td>\n",
       "      <td>S S S C S S S P</td>\n",
       "      <td>меня привезли в крепость,уцелевшую посереди сгоревшего города.</td>\n",
       "      <td>меня привезли в крепость, уцелевшую посереди сгоревшего города.</td>\n",
       "      <td>S S S C S S S P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>преследовать его было невозможно у михельсона не было и тридцати годных лошадей</td>\n",
       "      <td>S S S C S S S S S S S P</td>\n",
       "      <td>преследовать его было невозможно,у михельсона не было и тридцати годных лошадей.</td>\n",
       "      <td>преследовать его было невозможно. у михельсона не было и тридцати годных лошадей.</td>\n",
       "      <td>S S S P S S S S S S S P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>пугачев выступил против князя голицына с десятью тысячами отборного войска оставя под оренбургом шигаева с двумя тысячами</td>\n",
       "      <td>S S S S S S S S S C S S S S S S P</td>\n",
       "      <td>пугачев выступил против князя голицына с десятью тысячами отборного войска,оставя под оренбургом шигаева с двумя тысячами.</td>\n",
       "      <td>пугачев выступил против князя голицына с десятью тысячами отборного войска, оставя под оренбургом шигаева с двумя тысячами.</td>\n",
       "      <td>S S S S S S S S S C S S S S S S P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>никто не знал что уже накануне михельсон в семи верстах от города имел жаркое дело с пугачевым и что мятежники отступили в беспорядке</td>\n",
       "      <td>S S C S S S S S S S S S S S S S S S S S S S P</td>\n",
       "      <td>никто не знал,что уже накануне михельсон в семи верстах от города имел жаркое дело с пугачевым и что мятежники отступили в беспорядке.</td>\n",
       "      <td>никто не знал, что уже накануне михельсон в семи верстах от города имел жаркое дело с пугачевым и что мятежники отступили в беспорядке.</td>\n",
       "      <td>S S C S S S S S S S S S S S S S S S S S S S P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      input  \\\n",
       "3851                                  через полчаса маша должна была навсегда оставить родительский дом свою комнату тихую девическую жизнь   \n",
       "1432                                                                          меня привезли в крепость уцелевшую посереди сгоревшего города   \n",
       "2566                                                        преследовать его было невозможно у михельсона не было и тридцати годных лошадей   \n",
       "2325              пугачев выступил против князя голицына с десятью тысячами отборного войска оставя под оренбургом шигаева с двумя тысячами   \n",
       "2545  никто не знал что уже накануне михельсон в семи верстах от города имел жаркое дело с пугачевым и что мятежники отступили в беспорядке   \n",
       "\n",
       "                                             target  \\\n",
       "3851                    S S S S S S S S C S C S S P   \n",
       "1432                                S S S C S S S P   \n",
       "2566                        S S S C S S S S S S S P   \n",
       "2325              S S S S S S S S S C S S S S S S P   \n",
       "2545  S S C S S S S S S S S S S S S S S S S S S S P   \n",
       "\n",
       "                                                                                                                               output_target  \\\n",
       "3851                                  через полчаса маша должна была навсегда оставить родительский дом,свою комнату,тихую девическую жизнь.   \n",
       "1432                                                                          меня привезли в крепость,уцелевшую посереди сгоревшего города.   \n",
       "2566                                                        преследовать его было невозможно,у михельсона не было и тридцати годных лошадей.   \n",
       "2325              пугачев выступил против князя голицына с десятью тысячами отборного войска,оставя под оренбургом шигаева с двумя тысячами.   \n",
       "2545  никто не знал,что уже накануне михельсон в семи верстах от города имел жаркое дело с пугачевым и что мятежники отступили в беспорядке.   \n",
       "\n",
       "                                                                                                                                       output  \\\n",
       "3851                                через полчаса маша должна была навсегда оставить родительский дом, свою комнату, тихую, девическую жизнь.   \n",
       "1432                                                                          меня привезли в крепость, уцелевшую посереди сгоревшего города.   \n",
       "2566                                                        преследовать его было невозможно. у михельсона не было и тридцати годных лошадей.   \n",
       "2325              пугачев выступил против князя голицына с десятью тысячами отборного войска, оставя под оренбургом шигаева с двумя тысячами.   \n",
       "2545  никто не знал, что уже накануне михельсон в семи верстах от города имел жаркое дело с пугачевым и что мятежники отступили в беспорядке.   \n",
       "\n",
       "                                        output_punc  \n",
       "3851                    S S S S S S S S C S C C S P  \n",
       "1432                                S S S C S S S P  \n",
       "2566                        S S S P S S S S S S S P  \n",
       "2325              S S S S S S S S S C S S S S S S P  \n",
       "2545  S S C S S S S S S S S S S S S S S S S S S S P  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81190ac-7740-488a-b1a9-9a00c48a65b7",
   "metadata": {},
   "source": [
    "### Metrics _from the box_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c7a55af0-4160-467b-8aee-49eaa0bdc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_PUNC = ['P', 'EX', 'Q']\n",
    "INTR_PUNC = ['S', 'C']\n",
    "\n",
    "NAMES_PUNC = {\n",
    "    'S': 'space (` `)',\n",
    "    'C': 'comma (`,`)',\n",
    "    'P': 'point (`.`)',\n",
    "    'EX': 'excl. (`!`)',\n",
    "    'Q': 'question (`?`)',\n",
    "    'U': 'other'\n",
    "}\n",
    "\n",
    "CLASSES = sorted(END_PUNC + INTR_PUNC)  # alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dcdf414b-83c5-4766-b20a-b3012803f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_separate_punct(target_vs_pred_df, pred_col_name='output_punc'):\n",
    "    test_all_punc_target = []  # list of all punctuation\n",
    "    test_all_punc_preds = []\n",
    "    \n",
    "    for target_this, predicted_this in zip(target_vs_pred_df['target'], target_vs_pred_df[pred_col_name]):\n",
    "        test_all_punc_target.extend(target_this.split(' '))\n",
    "        test_all_punc_preds.extend(predicted_this.split(' '))\n",
    "    \n",
    "    assert len(test_all_punc_target) == len(test_all_punc_preds)\n",
    "    \n",
    "    return test_all_punc_target, test_all_punc_preds\n",
    "\n",
    "\n",
    "def get_all_metrics(test_df, pred_col_name='output_punc'):\n",
    "    test_all_punc_target, test_all_punc_preds = return_separate_punct(\n",
    "        test_df, pred_col_name=pred_col_name\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(test_all_punc_target, test_all_punc_preds)\n",
    "    # precision = TP / (TP + FP)\n",
    "    precision = precision_score(test_all_punc_target, test_all_punc_preds, average=None, zero_division=np.nan)\n",
    "    # recall = TP / (TP + FN)\n",
    "    recall = recall_score(test_all_punc_target, test_all_punc_preds, average=None, zero_division=np.nan)\n",
    "    # f1 = 2TP / (2TP + FP + FN)\n",
    "    f1 = f1_score(test_all_punc_target, test_all_punc_preds, average=None)\n",
    "\n",
    "    # PRINT\n",
    "    metrics_names = ['Precision', 'Recall', 'F1 score']\n",
    "    metrics = {'Precision': precision, 'Recall': recall, 'F1 score': f1}\n",
    "    col_w = 16\n",
    "    \n",
    "    print(' ' * col_w + '|' + ''.join([f\"{NAMES_PUNC[token] + (col_w - len(NAMES_PUNC[token])) * ' '}|\" for token in CLASSES]))  # header\n",
    "    print(''.join(['-' * col_w + '|' for _ in range(len(CLASSES) + 1)]) )\n",
    "    for ind, metric_name in enumerate(metrics_names):\n",
    "        row = f\"{metric_name + (col_w - len(metric_name)) * ' '}|\"\n",
    "        for score in metrics[metric_name]:\n",
    "            score_str = f'{score:.6f}'\n",
    "            row += f\"{score_str + (col_w - len(score_str)) * ' '}|\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6a27c9b3-6fc0-49bd-a1d8-26db275f646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                |comma (`,`)     |excl. (`!`)     |point (`.`)     |question (`?`)  |space (` `)     |\n",
      "----------------|----------------|----------------|----------------|----------------|----------------|\n",
      "Precision       |0.913819        |0.405941        |0.763220        |0.567123        |0.975520        |\n",
      "Recall          |0.774770        |0.232955        |0.960040        |0.824701        |0.983370        |\n",
      "F1 score        |0.838569        |0.296029        |0.850390        |0.672078        |0.979430        |\n",
      "CPU times: user 362 ms, sys: 14.7 ms, total: 377 ms\n",
      "Wall time: 379 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_all_metrics(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b30a1d-ace3-46ba-b908-437ed1364802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71562a59-dc05-484b-8167-28a5b94bb200",
   "metadata": {},
   "source": [
    "## Additional training on our data (fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6489d-ae94-4e90-8d0f-29a5bb86cc87",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c6a834ac-9e1a-499d-907e-32d26cab65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Features, ClassLabel, Value, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2ee17ae8-63cd-4d52-8bb9-830ab3e816bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_ENTITY = {\n",
    "    'S': 'O',\n",
    "    'C': 'B-,',\n",
    "    'P': 'B-.',\n",
    "    'EX': 'B-!',\n",
    "    'Q': 'B-?',\n",
    "}\n",
    "\n",
    "ENTITY_2_ID = {\n",
    "    'O': 0,\n",
    "    'B-,': 1,\n",
    "    'B-.': 2,\n",
    "    'B-!': 3,\n",
    "    'B-?': 4,\n",
    "    'B-:': 5,\n",
    "    'B-...': 6\n",
    "}\n",
    "ID_2_ENTITY = {v: k for k, v in ENTITY_2_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e7a28c6f-253d-46f8-8a8d-27e164f54e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_random_state = 78\n",
    "test_ratio = 0.25\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    np.arange(dataset_df.shape[0]),\n",
    "    test_size=test_ratio,\n",
    "    random_state=splitting_random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "be90d9e9-073f-4858-8f22-aaea30717c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3342/3342 [00:00<00:00, 15012.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1114/1114 [00:00<00:00, 24204.84it/s]\n"
     ]
    }
   ],
   "source": [
    "my_dataset = {'train': [], 'test': []}\n",
    "\n",
    "for ds_part_name, ids in zip(('train', 'test'), (train_ids, test_ids)):\n",
    "    print(ds_part_name, len(ids), sep=': ')\n",
    "    \n",
    "    for i, df_ind in enumerate(tqdm(ids)):\n",
    "        item = {}\n",
    "        item['id'] = i\n",
    "        \n",
    "        ner_tags = []\n",
    "        for token in dataset_df.iloc[df_ind]['target'].split():\n",
    "            ner_tags.append(ENTITY_2_ID[TOKEN_2_ENTITY[token]])\n",
    "        item['ner_tags'] = ner_tags\n",
    "        \n",
    "        item['tokens'] = dataset_df.iloc[df_ind]['input'].split()\n",
    "    \n",
    "        my_dataset[ds_part_name].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a0a186e9-3755-466c-98b5-7f839b3fe9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_features = Features(\n",
    "    {\n",
    "        'id': Value(dtype='int64', id=None),\n",
    "        'ner_tags': Sequence(\n",
    "            feature=ClassLabel(\n",
    "                num_classes=len(ENTITY_2_ID),\n",
    "                names=list(ENTITY_2_ID.keys()),\n",
    "                names_file=None, id=None\n",
    "            ), length=-1, id=None\n",
    "        ),\n",
    "        'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bdd9f02c-711a-4505-994b-c41d7f100d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_dataset = Dataset.from_list(my_dataset['train'], features=my_features)\n",
    "my_test_dataset = Dataset.from_list(my_dataset['test'], features=my_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2f3fb4c1-677c-4658-be56-b64ece6145db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1001,\n",
       " 'ner_tags': [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       " 'tokens': ['вдруг',\n",
       "  'дубровский',\n",
       "  'вздрогнул',\n",
       "  'в',\n",
       "  'укреплении',\n",
       "  'сделалась',\n",
       "  'тревога',\n",
       "  'и',\n",
       "  'степка',\n",
       "  'просунул',\n",
       "  'к',\n",
       "  'нему',\n",
       "  'голову',\n",
       "  'в',\n",
       "  'окошко']}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_dataset[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f10014b6-4022-4a45-be8b-a6b1022b58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_test_dataset = DatasetDict(\n",
    "    {\n",
    "    'train': my_train_dataset,\n",
    "    'test': my_test_dataset,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "79f07a97-9596-4458-9cf3-3113bed237e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 7,\n",
       " 'ner_tags': [1, 1, 0, 0, 0, 2],\n",
       " 'tokens': ['оба', 'соединясь', 'поспешили', 'вслед', 'за', 'пугачевым']}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_test_dataset['train'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7307b1-93ce-4261-95ed-195f6922c92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e4521249-d9d5-4cce-925b-a92ad4160c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-,', 'B-.', 'B-!', 'B-?', 'B-:', 'B-...'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebf1f3-3008-4f63-a938-614a15cf8b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "956865c0-effd-4483-9467-69b1cccf9fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-,', 'B-.', 'B-!', 'B-?', 'B-:', 'B-...']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = my_train_dataset.features['ner_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44dcb4-e71d-4793-a098-64efaa47afbd",
   "metadata": {},
   "source": [
    "### From [tutorial](https://huggingface.co/docs/transformers/tasks/token_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f1f92919-8344-4673-ad36-898d8193c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "cfce7d1f-0b4a-4952-8f25-6b5cb073c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "13ae655d-cbec-4071-b29d-4c3e039fb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "aa1c8f06-9bfc-4185-8285-0663c695a383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'хлеба', 'ради', 'хри', '##ста', 'хлеба', '[SEP]']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = my_train_test_dataset['train'][0]\n",
    "tokenized_input = tokenizer(example['tokens'], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ebd89096-9828-49c0-8df7-7546e869bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b6072e34-c0f7-48c3-82ff-a40caa08d0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434f6063c31f4bc79bcc3ab561f2a50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820c3736a33645c18a3add4c5750e47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_my_ds = my_train_test_dataset.map(\n",
    "    tokenize_and_align_labels, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6080d3a0-d3cc-47ae-a4b1-f6588977a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18c0be-8a66-4f7a-b2b7-528cc25af46c",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e3a7665d-eb34-4404-b7dc-67093687934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "177cf551-f52a-407a-ae94-3ac456d26d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "44687ad4-352d-45c6-88ad-16bd69835c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-,', 'O', 'B-,', 'B-.']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example['ner_tags']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a79492df-b28d-436e-b7d3-9710a1ca109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47fe39-db3b-4a2c-9896-8f7bc560b065",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3037d329-e51d-4435-8ba2-c88f31695207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ba680248-97b0-4f49-b158-1b5f0ed21c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_my = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path, \n",
    "    num_labels=len(ID_2_ENTITY),\n",
    "    id2label=ID_2_ENTITY,\n",
    "    label2id=ENTITY_2_ID,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "55f28d23-bc51-4f3f-baeb-8d719b6d8e75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[290], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_finetuned_hf_punc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# per_device_train_batch_size=16,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# per_device_eval_batch_size=16,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_my,\n\u001b[1;32m     14\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m<string>:128\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics)\u001b[0m\n",
      "File \u001b[0;32m~/science-phd/git-projects/mini-projects/itmo_nlp-course-24/venv/lib/python3.8/site-packages/transformers/training_args.py:1641\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1636\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1641\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal_than_2_3)\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1652\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1665\u001b[0m ):\n",
      "File \u001b[0;32m~/science-phd/git-projects/mini-projects/itmo_nlp-course-24/venv/lib/python3.8/site-packages/transformers/training_args.py:2149\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/science-phd/git-projects/mini-projects/itmo_nlp-course-24/venv/lib/python3.8/site-packages/transformers/utils/generic.py:59\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/science-phd/git-projects/mini-projects/itmo_nlp-course-24/venv/lib/python3.8/site-packages/transformers/training_args.py:2055\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2055\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2056\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2058\u001b[0m         )\n\u001b[1;32m   2059\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='my_finetuned_hf_punc',\n",
    "    learning_rate=2e-5,\n",
    "    # per_device_train_batch_size=16,\n",
    "    # per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_my,\n",
    "    args=training_args,\n",
    "    train_dataset=my_train_test_dataset['train'],\n",
    "    eval_dataset=my_train_test_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700f23a-4c65-49f7-a1a4-39858fd1dc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e04c13-89bb-4d27-88f2-48f356865eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea105ba-79fb-4dfd-a69c-8f7aaf32dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
