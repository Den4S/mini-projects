{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb78d9cc-4b13-4bb5-9963-f2e2f81f820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b27639e-2a35-4d83-9f66-c9f22c7b4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7feae1-b8a8-43a2-8a79-5f121b4ee485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6455eb2-5305-469b-be0a-14b0a40aef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15782236-f7fe-44b4-b6ce-b199c926ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdea49b9-a133-441f-b955-934aa75873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73357ba4-e9fa-44d8-876f-10fbe4333e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04cc7d-ed51-4c69-b46d-65474f9b5418",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dad4fde-3c39-483d-9af9-6336882e6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56bc5d4-e555-4436-ae25-33b765261466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c946cca3-005c-4b69-af7b-2348ba2386e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774109f-c1cc-44bb-b374-2f646925e4f9",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7889fc0-3058-442f-b449-bdd3b81148eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be used by default. Shows best results on intrinsic evaluations.\n",
    "# Model was trained on large corpus of an literature (~150GB).\n",
    "\n",
    "# !wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9724fa51-d904-43ea-a293-4bf72d1dcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dce022-5e2a-4587-90cd-ad959a159340",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b60ca58-b91d-4da6-af70-340573dc2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ceebadd-c435-4d31-8882-b0e2ab4e456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243e287-01bc-4ab7-b0c8-ca967519d21c",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea33e80d-d55f-48c2-a283-f1590f451eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45f5bf1-2006-4efe-8e46-64781b31bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "lables_fs = 16\n",
    "ticks_fs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d20c59-3418-4546-9d3c-190d19d038f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810418a-5395-4841-a942-8a3f6723ed86",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a5a7bce-8d67-4c98-8a84-8c1e91dc0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dir = '../data/prepared'\n",
    "filename_csv = '02_punct_pushkin.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d63ad3-3b17-4217-aefb-c25a5737672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4456, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved dataset\n",
    "data_df = pd.read_csv(os.path.join(prepared_dir, filename_csv), index_col=0)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d182437e-874a-4068-9ebd-d63755fbd039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>input_lemma</th>\n",
       "      <th>new_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>ну вот же вам ваши деньги отправляйтесь назад</td>\n",
       "      <td>ну вот же вы ваш деньга отправляться назад</td>\n",
       "      <td>S S S S S C S F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>эй дуня закричал смотритель поставь самовар да сходи за сливками</td>\n",
       "      <td>эй дуня закричать смотритель поставить самовар да сходить за сливка</td>\n",
       "      <td>C C S C S S S S S F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>к вечеру буря утихла и ветер оборотился в противную сторону</td>\n",
       "      <td>к вечер буря утихнуть и ветер оборотиться в противный сторона</td>\n",
       "      <td>S S S C S S S S S F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>он упал у колеса разбойники окружили его</td>\n",
       "      <td>он упасть у колесо разбойник окружить он</td>\n",
       "      <td>S S S C S S F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>пугачев ночевал на месте сражения на другой день занял дубовку и двинулся к царицыну</td>\n",
       "      <td>пугачев ночевать на место сражение на другой день занять дубовка и двинуться к царицын</td>\n",
       "      <td>S S S S C S S S S S S S S F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     input  \\\n",
       "3319                                         ну вот же вам ваши деньги отправляйтесь назад   \n",
       "4094                      эй дуня закричал смотритель поставь самовар да сходи за сливками   \n",
       "2540                           к вечеру буря утихла и ветер оборотился в противную сторону   \n",
       "3605                                              он упал у колеса разбойники окружили его   \n",
       "2690  пугачев ночевал на месте сражения на другой день занял дубовку и двинулся к царицыну   \n",
       "\n",
       "                                                                                 input_lemma  \\\n",
       "3319                                              ну вот же вы ваш деньга отправляться назад   \n",
       "4094                     эй дуня закричать смотритель поставить самовар да сходить за сливка   \n",
       "2540                           к вечер буря утихнуть и ветер оборотиться в противный сторона   \n",
       "3605                                                он упасть у колесо разбойник окружить он   \n",
       "2690  пугачев ночевать на место сражение на другой день занять дубовка и двинуться к царицын   \n",
       "\n",
       "                       new_target  \n",
       "3319              S S S S S C S F  \n",
       "4094          C C S C S S S S S F  \n",
       "2540          S S S C S S S S S F  \n",
       "3605                S S S C S S F  \n",
       "2690  S S S S C S S S S S S S S F  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "data_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd16deb-4f91-4555-9138-a4056097cf3a",
   "metadata": {},
   "source": [
    "### Pretrained Embeddings\n",
    "\n",
    "In that implementation we will use [`navec`](https://github.com/natasha/navec#evaluation) library of pretrained word embeddings for Russian language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04b578d7-aa87-4fad-b19d-e0fbfe000bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "navec_path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "navec_embed = Navec.load(navec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6ba7582-d88b-4bfe-833f-15f38e908a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ce0b2-7751-4e0f-ad43-0652b5ea488d",
   "metadata": {},
   "source": [
    "### DataFrame of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a91e294-dffd-4ab3-89ed-8acb1bc93bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_df(data_df):\n",
    "    bigrams_df = pd.DataFrame()  # dataframe of all bigrams\n",
    "    \n",
    "    bigrams_list = []\n",
    "    bigrams_punc_list = []\n",
    "    \n",
    "    for ind_row in tqdm(data_df.index):\n",
    "        sentence = data_df.loc[ind_row]['input'].split()\n",
    "        sent_len = len(sentence)\n",
    "    \n",
    "        sent_all_punc = data_df.loc[ind_row]['new_target'].split()\n",
    "    \n",
    "        assert len(sent_all_punc) == sent_len\n",
    "        \n",
    "        for ind_word in range(sent_len):\n",
    "            if ind_word == sent_len - 1:\n",
    "                bigram = ' '.join([sentence[ind_word], PAD])\n",
    "            else:\n",
    "                bigram = ' '.join([sentence[ind_word], sentence[ind_word + 1]])\n",
    "    \n",
    "            bigrams_list.append(bigram)\n",
    "            bigrams_punc_list.append(sent_all_punc[ind_word])\n",
    "    \n",
    "    bigrams_df['input'] = bigrams_list\n",
    "    bigrams_df['target'] = bigrams_punc_list\n",
    "\n",
    "    return bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07c4cd34-856f-43fb-a204-2f19b194e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4456/4456 [00:00<00:00, 22117.63it/s]\n"
     ]
    }
   ],
   "source": [
    "bigrams_df = get_bigrams_df(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a362f61c-315a-478a-9afa-25796a149c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39480</th>\n",
       "      <td>троекурову отымают</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38529</th>\n",
       "      <td>здравствуй володька</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744</th>\n",
       "      <td>сказать что</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52742</th>\n",
       "      <td>почталион погорельского</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30209</th>\n",
       "      <td>удачно &lt;pad&gt;</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44020</th>\n",
       "      <td>но дубровского</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51889</th>\n",
       "      <td>чувствуя затруднительность</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>я в</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56610</th>\n",
       "      <td>уж вас</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6847</th>\n",
       "      <td>удержания их</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            input target\n",
       "39480          троекурову отымают      C\n",
       "38529         здравствуй володька      C\n",
       "12744                 сказать что      C\n",
       "52742     почталион погорельского      S\n",
       "30209                удачно <pad>      F\n",
       "44020              но дубровского      S\n",
       "51889  чувствуя затруднительность      S\n",
       "2145                          я в      S\n",
       "56610                      уж вас      S\n",
       "6847                 удержания их      S"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13ea8e-580b-4f52-8410-fdccc73edc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a64abb-cdcf-4e80-aae6-2d5fc2cd3a1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "**In that approach** we will use _original_ (not lemmatized) sentences as input (`input` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6840e3d-77ec-4023-aec6-9115a2ae506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_ID = -1  # token id to ignore\n",
    "\n",
    "# punctuation vocab\n",
    "PUNC_2_ID = {'S': 0, 'C': 1, 'F':2}\n",
    "ID_2_PUNC = {v: k for k, v in PUNC_2_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7540fb6-a26f-4e09-b9ba-eb9b2ef9ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for punctuation prediction\"\"\"\n",
    "\n",
    "    def __init__(self, df, input_col, target_col, embed):\n",
    "        self.bigrams = df[input_col]  # all sentences\n",
    "        self.targets = df[target_col]  # all targets\n",
    "\n",
    "        self.embed = embed  # navec embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of sentences\"\"\"\n",
    "        return len(self.bigrams)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return one Tensor pair of (input id sequence, punc id sequence)\"\"\"\n",
    "        bigram = self.bigrams[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        words_seq, punc_id = self._preprocess(bigram, target)\n",
    "        return words_seq, punc_id\n",
    "\n",
    "    def _preprocess(self, bigram, target):\n",
    "        \"\"\"Convert txt sequence to word-id-seq and punc-id-seq\"\"\"\n",
    "        # INPUT\n",
    "        input_tensor = None\n",
    "        \n",
    "        for word in bigram.split():\n",
    "            if word in self.embed:  # if word in vocab\n",
    "                word_embed = navec_embed[word]\n",
    "            else:\n",
    "                word_embed = navec_embed[UNK]\n",
    "    \n",
    "            assert len(word_embed) == EMBED_SIZE\n",
    "            assert isinstance(word_embed, np.ndarray)\n",
    "        \n",
    "            word_embed_tensor = torch.tensor(word_embed)  # size: [embed_size]\n",
    "            word_embed_tensor = word_embed_tensor.unsqueeze(dim=0)  # size: [1, embed_size]\n",
    "        \n",
    "            assert word_embed_tensor.size() == (1, EMBED_SIZE)\n",
    "        \n",
    "            if input_tensor is None:\n",
    "                input_tensor = word_embed_tensor\n",
    "            else:\n",
    "                input_tensor = torch.cat(\n",
    "                    (input_tensor, word_embed_tensor), \n",
    "                    dim=0\n",
    "                )\n",
    "        # size: [2, embed_size]\n",
    "        assert input_tensor.size() == (2, EMBED_SIZE)\n",
    "        \n",
    "        # OUTPUT\n",
    "        output = []\n",
    "        for punc in target.split():\n",
    "            output.append(PUNC_2_ID.get(punc))\n",
    "\n",
    "        return input_tensor, torch.LongTensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb84abd4-5c18-4d11-970a-872ff1a111e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58773\n"
     ]
    }
   ],
   "source": [
    "bigrams_ds = BigramsDataset(\n",
    "    df=bigrams_df, \n",
    "    input_col='input', \n",
    "    target_col='target',\n",
    "    embed=navec_embed\n",
    ")\n",
    "\n",
    "print(len(bigrams_ds))  # dataset length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e905e01-a0c1-48e6-b443-e999f60d027e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06b6d0a-03e7-4555-aa79-950a16e2ebbc",
   "metadata": {},
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b496b7c-40b1-4b1f-9af3-b8ee53e96851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    bigrams_ds,\n",
    "    batch_size=10, \n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c9c17a9-69ef-459c-934d-28bd6b93dbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 300])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader))[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463282f-5cd6-4632-9793-cde69f674fee",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "971f53cb-f007-4a32-8c7e-b453afcde54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramsClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_class, \n",
    "        hidden_size=64,\n",
    "        kernel_size=5,\n",
    "        stride=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_class = num_class\n",
    "        # Hyper-parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        left_padding = stride * (kernel_size - 1)\n",
    "        self.padding = nn.ZeroPad2d((left_padding, 0, 0, 0)) # left padding\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            in_channels=EMBED_SIZE,\n",
    "            out_channels=self.hidden_size, \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=self.stride\n",
    "        )\n",
    "        self.conv_2 = nn.Conv1d(\n",
    "            in_channels=self.hidden_size,\n",
    "            out_channels=self.hidden_size,\n",
    "            kernel_size=2,\n",
    "            # kernel_size=self.kernel_size,\n",
    "            # stride=self.stride\n",
    "        )\n",
    "\n",
    "        # self.conv_3 = nn.Conv1d(\n",
    "        #     in_channels=self.hidden_size,\n",
    "        #     out_channels=self.hidden_size // 2, \n",
    "        #     kernel_size=2,\n",
    "        # )\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_class)\n",
    "\n",
    "    def forward(self, embedded_input):\n",
    "        embedded_input = torch.permute(embedded_input, (0, 2, 1))\n",
    "        # print(f'input: {embedded_input.size()}')\n",
    "        \n",
    "        out = self.conv_1(self.padding(embedded_input))\n",
    "        # print(f'conv_1: {out.size()}')\n",
    "\n",
    "        # out = self.conv_2(self.padding(out))\n",
    "        out = self.conv_2(out)\n",
    "        # print(f'conv_2: {out.size()}')\n",
    "\n",
    "        # out = self.conv_3(out)\n",
    "        # print(f'conv_3: {out.size()}')\n",
    "        \n",
    "        out = torch.permute(out, (0, 2, 1))\n",
    "        out = self.fc(out)\n",
    "        # print(f'output: {out.size()}')\n",
    "        return out # output tensor should be of shape [batch_size, 2, n_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d996df5b-94c9-4a4a-b483-8b331396eb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "# test of conv1d\n",
    "a = torch.randn(15, 100, 2)  \n",
    "m = nn.Conv1d(100, 32, kernel_size=2, stride=1) \n",
    "out = m(a)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7124ef-fa84-4e4f-acff-7c25e2292966",
   "metadata": {},
   "source": [
    "## Training and Evaluationg loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f657e36-5cc4-4150-b702-365e88de5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, loss_func, optimizer,\n",
    "             device='cpu', show_process=False):\n",
    "    '''\n",
    "    Function to train `model`\n",
    "    Args:\n",
    "        model: torch.nn.Module - Neural Network\n",
    "        data_loader: torch.utils.data.DataLoader - loader (by batches) for the train dataset\n",
    "        loss_func - loss function\n",
    "        optimizer: torch.optim\n",
    "        device: str - device to computate on\n",
    "        show_process: bool - flag to show (or not) a progress bar\n",
    "    Returns:\n",
    "        mean loss by batches\n",
    "    '''\n",
    "    model.train()  # activate 'train' mode of a model\n",
    "    train_loss = []  # to store loss for each batch\n",
    "\n",
    "    for X, y in tqdm(data_loader, total=len(data_loader),\n",
    "                                desc='train', position=0,\n",
    "                                leave=True, disable=not show_process):  # [X, y] - batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(X)  # size: [bs, max_sent_length, num_classes]\n",
    "\n",
    "        # print(f'{y} vs {y_hat}')\n",
    "        \n",
    "        y_hat = y_hat.view(-1, y_hat.size(-1))\n",
    "        loss = loss_func(y_hat, y.view(-1))  # loss calculation for the batch\n",
    "        \n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())  # accumulate losses for batches\n",
    "\n",
    "    return np.mean(train_loss)  # return mean loss of the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c41a1eee-7086-44d4-84c6-5b5eb6a8efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fn(model, data_loader, loss_func,\n",
    "                device='cpu', show_process=False):\n",
    "    '''\n",
    "    Function to train `model`\n",
    "    Args:\n",
    "        model: torch.nn.Module - Neural Network\n",
    "        data_loader: torch.utils.data.DataLoader - loader (by batches) for the validation dataset\n",
    "        loss_func - loss function\n",
    "        device: str - device to computate on\n",
    "        show_process: bool - flag to show (or not) a progress bar\n",
    "    Returns:\n",
    "          mean loss by batches\n",
    "    '''\n",
    "    model.eval()  # activate 'eval' mode of a model\n",
    "    val_loss = []  # to store loss for each batch\n",
    "\n",
    "    for X, y in tqdm(data_loader, total=len(data_loader),\n",
    "                                desc='validation', position=0,\n",
    "                                leave=True, disable=not show_process):  # [X, y] - batch\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X)  # size: [bs, max_sent_length, num_classes]\n",
    "\n",
    "            y_hat = y_hat.view(-1, y_hat.size(-1))\n",
    "            loss = loss_func(y_hat, y.view(-1))  # loss calculation for the batch\n",
    "\n",
    "        val_loss.append(loss.item())  # accumulate losses for batches\n",
    "\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c444626-8de5-40e5-8670-947272b3a430",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9df44b-8667-4a29-bfbc-ae92117aa2c6",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a8332de-a8ce-482c-829a-b80d5f5ae01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_random_state = 78\n",
    "test_ratio = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2f8fcb9-96e0-4e9c-95c3-da9f42c314e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "train_df, test_df = train_test_split(\n",
    "    data_df, \n",
    "    test_size=test_ratio, \n",
    "    random_state=splitting_random_state\n",
    ")\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6e9076f-78b0-421b-b356-4e52417cb243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3342/3342 [00:00<00:00, 25928.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44358"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_train_df = get_bigrams_df(train_df)\n",
    "len(bigrams_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35195d0c-adef-4ea6-84a8-65cc5358fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1114/1114 [00:00<00:00, 15730.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14415"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_test_df = get_bigrams_df(test_df)\n",
    "len(bigrams_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d67914ce-4e30-467b-a42d-ce3f35504169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7547343167781124"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigrams_train_df) / (len(bigrams_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e219916-0a05-4bbd-be2a-0cdbe9c55756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_ds = BigramsDataset(\n",
    "    df=bigrams_train_df, \n",
    "    input_col='input', \n",
    "    target_col='target',\n",
    "    embed=navec_embed\n",
    ")\n",
    "\n",
    "test_ds = BigramsDataset(\n",
    "    df=bigrams_test_df, \n",
    "    input_col='input', \n",
    "    target_col='target',\n",
    "    embed=navec_embed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "35ee8dce-f213-4e36-952f-a1e25f159eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = 250\n",
    "val_bs = 1000\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_bs, \n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=val_bs, \n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec685a78-230a-415a-9b93-c5662a53e1b3",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3d09e458-d97a-4df5-a8b8-8d3dabcb3e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "287764f3-a6ef-4841-9452-aca76a580a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "num_class = len(PUNC_2_ID)\n",
    "\n",
    "hidden_size = 32\n",
    "kernel_size = 3\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ec3afd75-b2fc-4e92-ab38-d85e8c359752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "model = BigramsClassifier(\n",
    "    num_class, \n",
    "    hidden_size,\n",
    "    kernel_size, stride\n",
    ").to(device)\n",
    "\n",
    "# criterion\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_ID)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "# scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    # factor=0.5,  # default: 0.1\n",
    "    # patience=2,  # default: 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cfe0a51e-b34d-464e-8927-4dd64e96475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: \ttrain - 0.564869; \tval - 0.482642\t\ttime - 4.767 s\n",
      "Epoch #2: \ttrain - 0.470776; \tval - 0.468384\t\ttime - 9.364 s\n",
      "Epoch #3: \ttrain - 0.458943; \tval - 0.465848\t\ttime - 13.904 s\n",
      "Epoch #4: \ttrain - 0.453823; \tval - 0.464603\t\ttime - 18.379 s\n",
      "Epoch #5: \ttrain - 0.450574; \tval - 0.463622\t\ttime - 23.338 s\n",
      "Epoch #6: \ttrain - 0.448198; \tval - 0.462771\t\ttime - 28.212 s\n",
      "Epoch #7: \ttrain - 0.446341; \tval - 0.462022\t\ttime - 32.721 s\n",
      "Epoch #8: \ttrain - 0.444836; \tval - 0.461363\t\ttime - 38.435 s\n",
      "Epoch #9: \ttrain - 0.443589; \tval - 0.460785\t\ttime - 44.494 s\n",
      "Epoch #10: \ttrain - 0.442539; \tval - 0.460281\t\ttime - 53.700 s\n",
      "Epoch #11: \ttrain - 0.441639; \tval - 0.459859\t\ttime - 65.438 s\n",
      "Epoch #12: \ttrain - 0.440861; \tval - 0.459519\t\ttime - 77.800 s\n",
      "Epoch #13: \ttrain - 0.440207; \tval - 0.459176\t\ttime - 89.745 s\n",
      "Epoch #14: \ttrain - 0.439577; \tval - 0.458992\t\ttime - 99.441 s\n",
      "Epoch #15: \ttrain - 0.438985; \tval - 0.459039\t\ttime - 109.344 s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "print_each = 1\n",
    "\n",
    "start_time = time.time()\n",
    "prev_val_loss = 100\n",
    "for epoch in range(n_epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    if (epoch == 0) or ((epoch + 1) % print_each == 0):\n",
    "        print(f'Epoch #{epoch + 1}: ', end='')\n",
    "\n",
    "    # torch.manual_seed(48)  # for reproducibility\n",
    "    mean_train_loss = train_fn(model, train_loader, loss_func,\n",
    "                               optimizer,\n",
    "                               device=device,\n",
    "                               show_process=False\n",
    "                              )  # train the model\n",
    "    mean_val_loss = validate_fn(model, val_loader, loss_func,\n",
    "                                device=device,\n",
    "                                show_process=False\n",
    "                               )  # evaluate the model\n",
    "    \n",
    "    if (epoch == 0) or ((epoch + 1) % print_each == 0):\n",
    "        log_info = (f'\\ttrain - {mean_train_loss:.6f}; ' +\n",
    "                    f'\\tval - {mean_val_loss:.6f}' + \n",
    "                    f'\\t\\ttime - {(time.time() - start_time):.3f} s'\n",
    "                   )\n",
    "        print(log_info)\n",
    "\n",
    "    if prev_val_loss < mean_val_loss:\n",
    "            break\n",
    "    prev_val_loss = mean_val_loss\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step(mean_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cec6b-a2c2-4daa-916e-7c6cfb73e74b",
   "metadata": {},
   "source": [
    "#### Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8e1cf862-2107-4438-8606-42ef57d88c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c7a55af0-4160-467b-8aee-49eaa0bdc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_PUNC = ['F']\n",
    "INTR_PUNC = ['S', 'C']\n",
    "\n",
    "NAMES_PUNC = {\n",
    "    'S': 'space (` `)',\n",
    "    'C': 'comma (`,`)',\n",
    "    'F': 'end of sent',\n",
    "}\n",
    "\n",
    "CLASSES = sorted(END_PUNC + INTR_PUNC)  # alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dcdf414b-83c5-4766-b20a-b3012803f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_df(model, test_ds):\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1, \n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )  # test DataLoader\n",
    "\n",
    "    all_test_targets = []  # by markers\n",
    "    all_test_preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (data) in enumerate(test_loader):\n",
    "        padded_input, padded_target = data\n",
    "        all_test_targets.append(' '.join([ID_2_PUNC[ix.item()] for ix in padded_target[0]]))\n",
    "        \n",
    "        pred = model(padded_input)\n",
    "        pred = torch.argmax(pred.view(-1, pred.size(-1)), dim=1)\n",
    "        all_test_preds.append(' '.join([ID_2_PUNC[ix.item()] for ix in pred]))\n",
    "\n",
    "        assert len(pred) == len(padded_target[0])\n",
    "\n",
    "    # DataFrame with results\n",
    "    target_vs_pred_df = pd.DataFrame()\n",
    "\n",
    "    target_vs_pred_df['target'] = all_test_targets\n",
    "    target_vs_pred_df['predicted'] = all_test_preds\n",
    "\n",
    "    return target_vs_pred_df\n",
    "\n",
    "\n",
    "def return_separate_punct(target_vs_pred_df):\n",
    "    test_all_punc_target = []  # list of all punctuation\n",
    "    test_all_punc_preds = []\n",
    "    \n",
    "    for target_this, predicted_this in zip(target_vs_pred_df['target'], target_vs_pred_df['predicted']):\n",
    "        test_all_punc_target.extend(target_this.split(' '))\n",
    "        test_all_punc_preds.extend(predicted_this.split(' '))\n",
    "    \n",
    "    assert len(test_all_punc_target) == len(test_all_punc_preds)\n",
    "    \n",
    "    return test_all_punc_target, test_all_punc_preds\n",
    "\n",
    "\n",
    "def get_all_metrics(model, test_df):\n",
    "    target_vs_pred_df = get_predictions_df(model, test_df)\n",
    "    test_all_punc_target, test_all_punc_preds = return_separate_punct(target_vs_pred_df)\n",
    "\n",
    "    cm = confusion_matrix(test_all_punc_target, test_all_punc_preds)\n",
    "    # precision = TP / (TP + FP)\n",
    "    precision = precision_score(test_all_punc_target, test_all_punc_preds, average=None, zero_division=np.nan)\n",
    "    # recall = TP / (TP + FN)\n",
    "    recall = recall_score(test_all_punc_target, test_all_punc_preds, average=None, zero_division=np.nan)\n",
    "    # f1 = 2TP / (2TP + FP + FN)\n",
    "    f1 = f1_score(test_all_punc_target, test_all_punc_preds, average=None)\n",
    "\n",
    "    # PRINT\n",
    "    metrics_names = ['Precision', 'Recall', 'F1 score']\n",
    "    metrics = {'Precision': precision, 'Recall': recall, 'F1 score': f1}\n",
    "    col_w = 18\n",
    "    \n",
    "    print(' ' * col_w + '|' + ''.join([f\"{NAMES_PUNC[token] + (col_w - len(NAMES_PUNC[token])) * ' '}|\" for token in CLASSES]))  # header\n",
    "    print(''.join(['-' * col_w + '|' for _ in range(len(CLASSES) + 1)]) )\n",
    "    for ind, metric_name in enumerate(metrics_names):\n",
    "        row = f\"{metric_name + (col_w - len(metric_name)) * ' '}|\"\n",
    "        for score in metrics[metric_name]:\n",
    "            score_str = f'{score:.6f}'\n",
    "            row += f\"{score_str + (col_w - len(score_str)) * ' '}|\"\n",
    "        print(row)\n",
    "\n",
    "    # Levenshtein distance\n",
    "    print('\\nLevenshtein distance:')\n",
    "    target_vs_pred_df['levenshtein'] = target_vs_pred_df.apply(\n",
    "        lambda row: levenshtein_distance(row.target, row.predicted),\n",
    "        axis = 1\n",
    "    )\n",
    "    print(f\"\\tMean: {target_vs_pred_df['levenshtein'].mean()}\")\n",
    "    print(f\"\\tMIN : {target_vs_pred_df['levenshtein'].min()}\")\n",
    "    print(f\"\\tMAX : {target_vs_pred_df['levenshtein'].max()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "44438b94-c305-475d-9bf3-578b823f9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  |comma (`,`)       |end of sent       |space (` `)       |\n",
      "------------------|------------------|------------------|------------------|\n",
      "Precision         |0.617602          |0.639752          |0.849541          |\n",
      "Recall            |0.352839          |0.647217          |0.924959          |\n",
      "F1 score          |0.449103          |0.643463          |0.885647          |\n",
      "\n",
      "Levenshtein distance:\n",
      "\tMean: 0.18806798473812\n",
      "\tMIN : 0\n",
      "\tMAX : 1\n",
      "\n",
      "CPU times: user 6.3 s, sys: 11.2 s, total: 17.5 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_all_metrics(model_test, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3eddf5-ead7-42ef-8ae3-473530b86a22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd363786-3127-49f7-8460-cfea120488c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
